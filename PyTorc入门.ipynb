{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何学好pytorch\n",
    " - 学好深度学习的基础知识\n",
    " - 学好PyTorch官方tutorial\n",
    " - 学习Github以及各种博客上的教程（别人创建好的list)\n",
    " - 阅读documentation,使用论坛https://discuss.pytorch.org/\n",
    " - 跑通以及学习开源PyTorch项目\n",
    " - 阅读深度学习模型paper,学习别人的模型实现\n",
    " - 通过阅读paper,自己实现模型\n",
    " - 自己创造模型（也可一些paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu 0.3857862949371338 tensor(140395.5156)\n"
     ]
    }
   ],
   "source": [
    "# 验证时间\n",
    "# print(torch.cuda.is_available())  # False 因为没有。。。。\n",
    "# \n",
    "\n",
    "a = torch.randn(10000,1000)\n",
    "b = torch.randn(1000,2000)\n",
    "\n",
    "t0 = time.time()\n",
    "c = torch.matmul(a,b)\n",
    "t1 = time.time()\n",
    "print(a.device,t1 - t0, c.norm(2))\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "# a = a.to(device)\n",
    "# b = b.to(device)\n",
    "# t2 = time.time()\n",
    "# print(a.device,t2 - t0, c.norm(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  None None None\n",
      "after : tensor(2.) tensor(1.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 一个简单公式的求导运算\n",
    "from torch import autograd\n",
    "\n",
    "x = torch.tensor(1.)\n",
    "a = torch.tensor(1., requires_grad=True)\n",
    "b = torch.tensor(2., requires_grad=True)\n",
    "c = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# 公式\n",
    "y = a**2 * x + b*x + c\n",
    "print('before: ',a.grad, b.grad, c.grad)\n",
    "grads = autograd.grad(y, [a,b,c])\n",
    "print('after :', grads[0], grads[1], grads[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Anaconda\n",
    "2. CUDA\n",
    "        测试：mvcc -V\n",
    "3. PyTorch安装\n",
    "        官网： http://pytorch.org\n",
    "4. pycharm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST \n",
    "\n",
    "        echo number owns 7000 images\n",
    "        train/test 60k/10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6103, -0.1381,  0.5713],\n",
       "        [ 0.9627,  1.7847,  0.3872]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.randn(2,3) # 随机正态分布\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(a.size(0))\n",
    "print(a.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6048, 0.1525, 0.4328],\n",
      "         [0.2932, 0.4741, 0.8417]]])\n",
      "tensor([[0.6048, 0.1525, 0.4328],\n",
      "        [0.2932, 0.4741, 0.8417]])\n",
      "a.shape: torch.Size([1, 2, 3])\n",
      "a.shape[0]: 1\n",
      "a.size(0): 1\n",
      "[1, 2, 3]\n",
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(1,2,3) # 随机均匀分布\n",
    "print(a)\n",
    "print(a[0])\n",
    "\n",
    "print('a.shape: ' + str(a.shape))\n",
    "print('a.shape[0]: ' + str(a.shape[0]))\n",
    "print('a.size(0): ' + str(a.size(0)))\n",
    "print(list(a.shape))\n",
    "print(a.numel())   # 1X2X3\n",
    "print(a.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 3.3000], dtype=torch.float64)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "tensor([2.0000, 3.2000])\n",
      "tensor([[4.6359e-26, 3.0889e-41, 2.7201e+23],\n",
      "        [2.1625e+00, 0.0000e+00, 0.0000e+00]])\n",
      "\n",
      "------------------------------\n",
      "\n",
      "tensor([4.4278e-26])\n",
      "tensor([[4.4539e-26, 3.0889e-41, 2.7201e+23],\n",
      "        [2.1625e+00, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([[ 358237424,      22043, -454521616],\n",
      "        [     32744,          0,          0]], dtype=torch.int32)\n",
      "tensor([[4.6359e-26, 3.0889e-41, 2.7201e+23],\n",
      "        [2.1625e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# 从numpy\n",
    "import numpy as np\n",
    "\n",
    "a=np.array([2,3.3])\n",
    "print(torch.from_numpy(a))\n",
    "\n",
    "print('\\n' + '-'*30 + '\\n')\n",
    "    \n",
    "# torch.tensor & torch.FloatTensor \n",
    "# tensor()  传入的参数是 data列表\n",
    "# FloatTensor()  大写的Tensor传入的是 shape (d1,d2) \n",
    "\n",
    "print(torch.tensor([2., 3.2]))\n",
    "print(torch.FloatTensor(2,3))\n",
    "\n",
    "print('\\n' + '-'*30 + '\\n')\n",
    "\n",
    "# 未初始化数据 uninitialized\n",
    "# 要用自己的数据覆盖掉 否则会出现问题 ，所初始化的数据并不是0 而是 nan，inf\n",
    "print(torch.empty(1))\n",
    "print(torch.Tensor(2,3))\n",
    "print(torch.IntTensor(2,3))\n",
    "print(torch.FloatTensor(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set default type\n",
    "\n",
    "```python\n",
    "torch.tensor([1.2,3]).type #　‘torch.FloatTensor’\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rand  rand_like randint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6990, 0.6400, 0.3875],\n",
      "        [0.2574, 0.2591, 0.1052],\n",
      "        [0.7650, 0.9496, 0.2416]])\n",
      "\n",
      "------------------------------\n",
      "\n",
      "tensor([[0.7429, 0.5448, 0.5964],\n",
      "        [0.2916, 0.4956, 0.0968],\n",
      "        [0.7257, 0.4174, 0.3393]])\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[8, 2, 3],\n",
       "        [6, 1, 3],\n",
       "        [8, 8, 7]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rand  rand_like randint \n",
    "\n",
    "# rand [0,1] 均匀分布\n",
    "print(torch.rand(3,3))\n",
    "a = torch.rand(3,3)\n",
    "\n",
    "print('\\n' + '-'*30 + '\\n')\n",
    "\n",
    "# rand_like\n",
    "print(torch.rand_like(a))\n",
    "\n",
    "print('\\n' + '-'*30 + '\\n')\n",
    "\n",
    "# randint(min,max,[shape]) 不包含ｍａｘ值\n",
    "torch.randint(1,10,[3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full randn arange range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7., 7., 7.],\n",
      "        [7., 7., 7.]])\n",
      "tensor(7.)\n",
      "tensor([7.])\n"
     ]
    }
   ],
   "source": [
    "# torch.full([shape], num)\n",
    "print(torch.full([2,3],7))\n",
    "\n",
    "print(torch.full([],7)) # 标量\n",
    "　\n",
    "print(torch.full([1],7))　　# 向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4870, -0.0583, -0.8997],\n",
      "        [-1.0850,  1.5969, -1.7848],\n",
      "        [ 0.3626,  1.8062,  0.7521]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5630, -0.0539, -0.3411, -1.4008, -0.5734,  0.1770,  0.1998, -0.5150,\n",
       "         0.1741,  0.0118])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randn\n",
    "\n",
    "# N(0,1)\n",
    "print(torch.randn(3,3))\n",
    "\n",
    "torch.normal(mean=torch.full([10],0), std=torch.arange(1,0,-0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# range arange\n",
    "print(torch.arange(0,10)) # [0,10)\n",
    "print(torch.arange(0,10,2))\n",
    "\n",
    "# range　不支持使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linspace  logspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  3.3333,  6.6667, 10.0000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linspace(min,max,steps) 从min-max 分成steps份　，包括　min max\n",
    "\n",
    "torch.linspace(0,10,steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.4642, 0.2154, 0.1000])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10^min - 10^max steps份\n",
    "\n",
    "torch.logspace(0,-1,steps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ones zeros eye对角"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# ones() ones_like()\n",
    "print(torch.ones(3,3))\n",
    "\n",
    "# zeros()\n",
    "print(torch.zeros(3,3))\n",
    "\n",
    "# eye()\n",
    "print(torch.eye(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randperm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2334, 0.6470, 0.3559],\n",
      "        [0.7644, 0.7731, 0.5610]])\n",
      "tensor([1, 0])\n",
      "tensor([[0.7644, 0.7731, 0.5610],\n",
      "        [0.2334, 0.6470, 0.3559]])\n"
     ]
    }
   ],
   "source": [
    "# 与　random.shuffle 相似\n",
    "# randperm(10)  生成是个随机索引，不包含１０\n",
    "torch.randperm(10)\n",
    "\n",
    "a=torch.rand(2,3)\n",
    "print(a)\n",
    "idx = torch.randperm(2)\n",
    "print(idx)\n",
    "print(a[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape: torch.Size([4, 3, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基于　：　的切片方式　　无\n",
    "\n",
    "# index_select(dim, tensor)  \n",
    "\n",
    "a = torch.rand([4,3,28,28])\n",
    "print(\"a.shape: \" + str(a.shape))\n",
    "\n",
    "# b = torch.tensor([0,2])\n",
    "# a.index_select(0, b).shape 　传入的ｂ要为一个tensor\n",
    "\n",
    "a.index_select(2,torch.arange(28)).shape\n",
    "# torch.Size([4, 3, 28, 28])\n",
    "\n",
    "a.index_select(2,torch.arange(8)).shape\n",
    "# torch.Size([4, 3, 8, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 14])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... \n",
    "\n",
    "a[...].shape\n",
    "# torch.Size([4, 3, 28, 28])\n",
    "\n",
    "a[0,...].shape\n",
    "# torch.Size([3, 28, 28])\n",
    "\n",
    "a[:,1,...].shape\n",
    "# torch.Size([4, 28, 28])\n",
    "\n",
    "a[...,:2].shape\n",
    "# torch.Size([4, 3, 28, 2])\n",
    "\n",
    "a[0,...,::2].shape\n",
    "# torch.Size([3, 28, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masked_select(x, mask)   x,tensor    mask掩码\n",
    "\n",
    "\n",
    "x = torch.randn(3,4)\n",
    "\n",
    "mask = x.ge(0.5)\n",
    "# tensor([[False, False,  True, False],\n",
    "#         [False,  True,  True, False],\n",
    "#         [False, False, False, False]])\n",
    "\n",
    "torch.masked_select(x, mask)\n",
    "# tensor([0.7614, 1.0010, 0.6398, 1.0508, 1.0553])\n",
    "\n",
    "torch.masked_select(x, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.tensor([[4,3,5],\n",
    "                   [6,7,8]])\n",
    "# take将　src(2,3) -> (1,6)\n",
    "torch.take(src, torch.tensor([0,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 维度变换\n",
    "      \n",
    "      View/reshape\n",
    "      Squeeze/unsqueeze　挤压\n",
    "      Transpose/t/permute\n",
    "      Expand/repeat　　broadcasting/memory copied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4620, 0.8063, 0.2376,  ..., 0.8099, 0.2015, 0.0292],\n",
       "        [0.5138, 0.7195, 0.0693,  ..., 0.5622, 0.9657, 0.1176],\n",
       "        [0.4308, 0.5260, 0.1161,  ..., 0.4229, 0.2340, 0.3305],\n",
       "        [0.9029, 0.3899, 0.9379,  ..., 0.4319, 0.2347, 0.8156]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view / reshape 都是相同的\n",
    "# view是在早期版本的提供的ａｐｉ\n",
    "\n",
    "\n",
    "# logic bug\n",
    "a=torch.rand(4,1,28,28)\n",
    "a.view(4,784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### squeeze unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# squeeze挤压　减少维度 unsqueeze增加维度\n",
    "\n",
    "a = torch.rand([4,1,28,28])\n",
    "\n",
    "# 正数之前插入\n",
    "\n",
    "a.unsqueeze(0).shape\n",
    "# torch.Size([1, 4, 1, 28, 28])\n",
    "\n",
    "a.unsqueeze(2).shape\n",
    "# torch.Size([4, 1, 1, 28, 28])\n",
    "\n",
    "a.unsqueeze(4).shape\n",
    "# torch.Size([4, 1, 28, 28, 1])\n",
    "\n",
    "# 负数之后插入\n",
    "\n",
    "a.unsqueeze(-1).shape # -1　代表最后一个元素位置\n",
    "\n",
    "a.unsqueeze(-5).shape\n",
    "# torch.Size([1, 4, 1, 28, 28])\n",
    "\n",
    "\n",
    "# example\n",
    "b = torch.rand(32)\n",
    "f = torch.rand(4,32,14,14)\n",
    "b = b.unsqueeze(1).unsqueeze(2).unsqueeze(0)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# squeeze\n",
    "# 压缩　去掉维数为１的维度\n",
    "# squeeze(a) 将ａ中所有为１的维度删除\n",
    "# squeeze(a,N) ａ中去掉指定的维数为一的维度\n",
    "\n",
    "b = torch.rand([1,32,1,1])\n",
    "\n",
    "b.squeeze().shape\n",
    "# torch.Size([32])\n",
    "\n",
    "b.squeeze(0).shape\n",
    "# torch.Size([32, 1, 1])\n",
    "\n",
    "b.squeeze(-1).shape\n",
    "# torch.Size([1, 32, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repeat expend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# repeat重复 \n",
    "b = torch.rand([1,32,1,1])  # 传入的是重复的次数\n",
    "b.shape\n",
    "# torch.Size([1, 32, 1, 1])\n",
    "\n",
    "b.repeat(4,32,1,1).shape\n",
    "# torch.Size([4, 1024, 1, 1])\n",
    "\n",
    "b.repeat(4,1,1,1).shape\n",
    "# torch.Size([4, 32, 1, 1])\n",
    "\n",
    "\n",
    "# expand \n",
    "# 返回tensor的新视图　不需要分配新内存　推荐\n",
    "\n",
    "a = torch.rand(4,32,14,14)\n",
    "\n",
    "# b torch.Size([1, 32, 1, 1])\n",
    "\n",
    "b.expand(4,32,14,14).shape\n",
    "# torch.Size([4, 32, 14, 14])\n",
    "\n",
    "b.expand(-1,32,-1,-1).shape # -1表示　默认\n",
    "# torch.Size([1, 32, 1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t transpose permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t矩阵转置　　只适用　2D tensor。\n",
    "\n",
    "a=torch.rand(3,4)\n",
    "a.t().shape\n",
    "# torch.Size([4, 3])\n",
    "\n",
    "#  transpose \n",
    "\n",
    "a=torch.rand(4,3,32,32)   #B　C H W\n",
    "\n",
    "# a1 = a.transpose(1,3).view(4,3*32*32).view(4,3,32,32)\n",
    "# a1.shape 4, 32, 32, 3  B W H C\n",
    "\n",
    "# contiguous()  重新申请内存，让数据连续\n",
    "a1 = a.transpose(1,3).contiguous().view(4,3*32*32).view(4,3,32,32)\n",
    "\n",
    "a2 = a.transpose(1,3).contiguous().view(4,3*32*32).view(4,32,32,3).transpose(1,3)\n",
    "\n",
    "a1.shape # [4, 3, 32, 32]\n",
    "a2.shape # [4, 3, 32, 32]\n",
    "\n",
    "torch.all(torch.eq(a,a1)) # tensor(False)\n",
    "\n",
    "torch.all(torch.eq(a,a2)) # tensor(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 28, 32, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(4,3,28,28)\n",
    "\n",
    "a.transpose(1,3).shape # 4, 28, 28, 3\n",
    "\n",
    "b = torch.rand(4,3,28,32)\n",
    "b.transpose(1,3).shape  # 4, 32, 28, 3\n",
    "b.transpose(1,3).transpose(1,2).shape  # 4, 28, 32, 3\n",
    "\n",
    "b.permute(0,2,3,1).shape # 4, 28, 32, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### broadcast 自动扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expand    without copying data\n",
    "\n",
    "        Insert 1 dim ahead\n",
    "        \n",
    "        expand dims with size 1 to same size\n",
    "        \n",
    "               \n",
    "for actual demanding \n",
    "\n",
    "memory consumption\n",
    "\n",
    "\n",
    "**match from last dim**\n",
    "        \n",
    "        if current dim =1  expand to same\n",
    "        if either has no dim.insert one dim and expand to same\n",
    "        otherwise, NOT broadcasting-able"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拼接\n",
    "\n",
    "       cat \n",
    "       stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cat\n",
    "\n",
    "a = torch.rand(4,32,8)\n",
    "b = torch.rand(5,32,8)\n",
    "\n",
    "torch.cat([a,b],dim=0).shape # torch.Size([9, 32, 8])\n",
    "\n",
    "\n",
    "# stack create new dim\n",
    "\n",
    "a = torch.rand(32,8)\n",
    "b = torch.rand(32,8)\n",
    "torch.stack([a,b],dim=0).shape # torch.Size([2, 32, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拆分\n",
    "\n",
    "    split\n",
    "    chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split ,by len\n",
    "# 长度相同，给定长度\n",
    "# 长度不同，给定一个list\n",
    "c = torch.rand(3,32,8)\n",
    "aa,bb,cc = c.split(1,dim=0)\n",
    "a1,b1 = c.split([2,1],dim=0)\n",
    "\n",
    "# chunk ,by num\n",
    "zz,xx = c.chunk(2,dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.add(a,b) +\n",
    "\n",
    "torch.sub(a,b) -\n",
    "\n",
    "torch.mul(a,b) *\n",
    "\n",
    "torch.div(a,b) /\n",
    "\n",
    "torch.matmul()  矩阵相乘  torch.mm() @\n",
    "\n",
    "a.power() 次方\n",
    "\n",
    "a.sqrt() 平方根\n",
    "\n",
    "a.rsqrt() 平方根的倒数\n",
    "\n",
    "torch.log(a) 默认e为底\n",
    "\n",
    "Approximation：\n",
    "        \n",
    "        floor() ceil() trunc() frac()\n",
    "        round() 近似\n",
    "        \n",
    "clamp : gradient clipping\n",
    "        \n",
    "        grad.clamp(）\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norm\n",
    "\n",
    "mean sum min max prod\n",
    "\n",
    "argmax()  argmin 返回索引\n",
    "\n",
    "dim  keepdim\n",
    "\n",
    "topk kthvalue\n",
    "\n",
    "eq返回tensor equal返回True or False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### advanced operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.where(condition,x,y) -> tensor\n",
    "\n",
    "    out = x_i if condition_i else y_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.gather(input,dim,index.out=None) -> Tensor 采集查表操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
